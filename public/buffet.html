<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8" />
  <title>üçΩÔ∏è Buffet ‚Äì Face Recognition</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <script src="/socket.io/socket.io.js"></script>
  <script src="/face-api.min.js"></script>

  <style>
    body { margin:0;font-family:Arial;background:#000;color:#fff;text-align:center;}
    h1{margin:10px 0;font-size:22px;}
    select,button{padding:10px;margin:6px;font-size:16px;border-radius:8px;border:none;}
    button{background:#0a7;color:#fff;}
    #videoBox{margin:10px auto;width:90%;max-width:360px;border:4px solid #0f0;border-radius:16px;box-shadow:0 0 20px #0f0;}
    video,canvas{width:100%;border-radius:12px;}
    #status{margin-top:8px;font-size:14px;color:#0f0;}
  </style>
</head>

<body>
<h1>üçΩÔ∏è BUFFET EXPERIENCE</h1>

<select id="cameraSelect"></select><br>
<button id="startCamBtn">üé• B·∫≠t camera</button>

<div id="videoBox">
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas" style="display:none;"></canvas>
</div>

<button id="confirmBtn" disabled>‚úÖ X√°c nh·∫≠n</button>
<div id="status">Ch∆∞a b·∫≠t camera</div>

<script>
const socket = io();
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");
const cameraSelect = document.getElementById("cameraSelect");
const statusText = document.getElementById("status");

let currentStream = null;

async function loadModels(){
  await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
  await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
  await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
}
loadModels();

async function loadCameras(){
  const devices = await navigator.mediaDevices.enumerateDevices();
  const cams = devices.filter(d=>d.kind==="videoinput");
  cameraSelect.innerHTML="";
  cams.forEach((c,i)=>{
    const o=document.createElement("option");
    o.value=c.deviceId; o.text=c.label||`Camera ${i+1}`;
    cameraSelect.appendChild(o);
  });
}

document.getElementById("startCamBtn").onclick = async ()=>{
  if(currentStream) currentStream.getTracks().forEach(t=>t.stop());
  const id = cameraSelect.value;
  currentStream = await navigator.mediaDevices.getUserMedia({video:{deviceId:{exact:id}}});
  video.srcObject = currentStream;
  document.getElementById("confirmBtn").disabled=false;
  statusText.textContent="üì∏ Gi·ªØ m·∫∑t trong khung 1 gi√¢y r·ªìi b·∫•m X√°c nh·∫≠n";
};

document.getElementById("confirmBtn").onclick = async ()=>{
  statusText.textContent="‚è≥ ƒêang nh·∫≠n di·ªán...";

  let bestDet = null;
  for(let i=0;i<3;i++){
    const d = await faceapi
      .detectSingleFace(video,new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptor();
    if(d) bestDet = d;
  }

  if(!bestDet){
    statusText.textContent="‚ùå Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t";
    return;
  }

  canvas.width=video.videoWidth;
  canvas.height=video.videoHeight;
  ctx.drawImage(video,0,0);

  socket.emit("action_try",{
    action:"buffet",
    image: canvas.toDataURL("image/jpeg"),
    descriptor: Array.from(bestDet.descriptor)
  });

  console.log("üçΩÔ∏è Buffet sent with descriptor");
};

socket.on("action_result", res=>{
  if(res.ok){
    statusText.textContent="‚úÖ X√°c nh·∫≠n th√†nh c√¥ng";
  }else{
    statusText.textContent="‚ùå " + res.reason;
  }
});

loadCameras();
</script>
</body>
</html>
